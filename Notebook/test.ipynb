{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7SfPUWXx0D9f"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tensorboardX\n",
    "!pip install efficientnet_pytorch\n",
    "!pip gdown"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fHwR_cMX0Mt7",
    "outputId": "429df1e9-9b92-4eda-ca36-91f2f84168d5"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/Stanford997/CS599-Fake-Video-Detection.git"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CsoWD_8n0OnC",
    "outputId": "7d81c490-6cb7-4499-fc81-7221d6ee84a4"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'CS599-Fake-Video-Detection'...\n",
      "remote: Enumerating objects: 48, done.\u001B[K\n",
      "remote: Counting objects: 100% (27/27), done.\u001B[K\n",
      "remote: Compressing objects: 100% (23/23), done.\u001B[K\n",
      "remote: Total 48 (delta 3), reused 22 (delta 3), pack-reused 21\u001B[K\n",
      "Receiving objects: 100% (48/48), 81.44 MiB | 47.98 MiB/s, done.\n",
      "Resolving deltas: 100% (3/3), done.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "cd CS599-Fake-Video-Detection"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z0Onw_lk0P8-",
    "outputId": "ea339489-fe3c-41be-80ac-b2a04ee1cd7b"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/CS599-Fake-Video-Detection\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!git pull https://github.com/Stanford997/CS599-Fake-Video-Detection.git"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0QaOetBJ0RL5",
    "outputId": "78213121-e6d5-4b7c-98c7-2d4863c1a5fa"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "From https://github.com/Stanford997/CS599-Fake-Video-Detection\n",
      " * branch            HEAD       -> FETCH_HEAD\n",
      "Already up to date.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from architectures import fornet\n",
    "from architectures.fornet import FeatureExtractor\n",
    "from isplutils import utils, split\n",
    "from isplutils.data import FrameFaceDatasetTest"
   ],
   "metadata": {
    "id": "PVpAr2TS0SYe"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Full path of the trained model\n",
    "model_path = Path('/content/drive/MyDrive/CS599/log/Xception/weights/net-Xception_traindb-ff-c23-720-140-140_face-scale_size-224_seed-41/bestval.pth')\n",
    "# Path to the Pandas Dataframe obtained from extract_faces.py on the FF++ dataset\n",
    "ffpp_df_path = '/content/drive/MyDrive/CS599/Dataset/image_df/df.pkl'\n",
    "# Path to the directory containing the faces extracted from the FF++ dataset\n",
    "ffpp_faces_dir = '/content/drive/MyDrive/CS599/Dataset/image'\n",
    "# Output folder\n",
    "results_dir = Path('results/')\n",
    "test_sets = ['ff-c23-720-140-140']\n",
    "device = 0\n",
    "num_workers = 6\n",
    "batch_size = 256\n",
    "max_num_videos_per_label = None\n",
    "debug = False\n",
    "override = False\n",
    "test_splits = ['val', 'test']\n",
    "dfdc_df_path = None\n",
    "dfdc_faces_dir = None"
   ],
   "metadata": {
    "id": "Xj-DpA4_0Tww"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def process_dataset(df: pd.DataFrame,\n",
    "                    root: str,\n",
    "                    net: FeatureExtractor,\n",
    "                    criterion,\n",
    "                    patch_size: int,\n",
    "                    face_policy: str,\n",
    "                    transformer: A.BasicTransform,\n",
    "                    batch_size: int,\n",
    "                    num_workers: int,\n",
    "                    device: torch.device,\n",
    "                    ) -> dict:\n",
    "    if isinstance(device, (int, str)):\n",
    "        device = torch.device(device)\n",
    "\n",
    "    dataset = FrameFaceDatasetTest(\n",
    "        root=root,\n",
    "        df=df,\n",
    "        size=patch_size,\n",
    "        scale=face_policy,\n",
    "        transformer=transformer,\n",
    "    )\n",
    "\n",
    "    # Preallocate\n",
    "    score = np.zeros(len(df))\n",
    "    loss = np.zeros(len(df))\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False)\n",
    "    with torch.no_grad():\n",
    "        idx0 = 0\n",
    "        for batch_data in tqdm(loader):\n",
    "            batch_images = batch_data[0].to(device)\n",
    "            batch_labels = batch_data[1].to(device)\n",
    "            batch_samples = len(batch_images)\n",
    "            batch_out = net(batch_images)\n",
    "            batch_loss = criterion(batch_out, batch_labels)\n",
    "            score[idx0:idx0 + batch_samples] = batch_out.cpu().numpy()[:, 0]\n",
    "            loss[idx0:idx0 + batch_samples] = batch_loss.cpu().numpy()[:, 0]\n",
    "            idx0 += batch_samples\n",
    "\n",
    "    out_dict = {'score': score, 'loss': loss}\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def select_videos(df: pd.DataFrame, max_videos_per_label: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Select up to a maximum number of videos\n",
    "    :param df: DataFrame of frames. Required columns: 'video','label'\n",
    "    :param max_videos_per_label: maximum number of real and fake videos\n",
    "    :return: DataFrame of selected frames\n",
    "    \"\"\"\n",
    "    # Save random state\n",
    "    st0 = np.random.get_state()\n",
    "    # Set seed for this selection only\n",
    "    np.random.seed(42)\n",
    "\n",
    "    df_fake = df[df.label == True]\n",
    "    fake_videos = df_fake['video'].unique()\n",
    "    selected_fake_videos = np.random.choice(fake_videos, min(max_videos_per_label, len(fake_videos)), replace=False)\n",
    "    df_selected_fake_frames = df_fake[df_fake['video'].isin(selected_fake_videos)]\n",
    "\n",
    "    df_real = df[df.label == False]\n",
    "    real_videos = df_real['video'].unique()\n",
    "    selected_real_videos = np.random.choice(real_videos, min(max_videos_per_label, len(real_videos)), replace=False)\n",
    "    df_selected_real_frames = df_real[df_real['video'].isin(selected_real_videos)]\n",
    "    # Restore random state\n",
    "    np.random.set_state(st0)\n",
    "\n",
    "    return pd.concat((df_selected_fake_frames, df_selected_real_frames), axis=0, verify_integrity=True).copy()"
   ],
   "metadata": {
    "id": "QAGTE8e00U2l"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# get arguments from the model path\n",
    "face_policy = str(model_path).split('face-')[1].split('_')[0]\n",
    "patch_size = int(str(model_path).split('size-')[1].split('_')[0])\n",
    "net_name = str(model_path).split('net-')[1].split('_')[0]\n",
    "model_name = '_'.join(model_path.with_suffix('').parts[-2:])\n",
    "\n",
    "# Load net\n",
    "net_class = getattr(fornet, net_name)\n",
    "\n",
    "# load model\n",
    "print('Loading model...')\n",
    "state_tmp = torch.load(model_path, map_location='cpu')\n",
    "if 'net' not in state_tmp.keys():\n",
    "    state = OrderedDict({'net': OrderedDict()})\n",
    "    [state['net'].update({'model.{}'.format(k): v}) for k, v in state_tmp.items()]\n",
    "else:\n",
    "    state = state_tmp\n",
    "net: FeatureExtractor = net_class().eval().to(device)\n",
    "\n",
    "incomp_keys = net.load_state_dict(state['net'], strict=True)\n",
    "print(incomp_keys)\n",
    "print('Model loaded!')\n",
    "\n",
    "# val loss per-frame\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "# Define data transformers\n",
    "test_transformer = utils.get_transformer(face_policy, patch_size, net.get_normalizer(), train=False)\n",
    "\n",
    "# datasets and dataloaders (from train_binclass.py)\n",
    "print('Loading data...')\n",
    "# Check if paths for DFDC and FF++ extracted faces and DataFrames are provided\n",
    "for dataset in test_sets:\n",
    "    if dataset.split('-')[0] == 'dfdc' and (dfdc_df_path is None or dfdc_faces_dir is None):\n",
    "        raise RuntimeError('Specify DataFrame and directory for DFDC faces for testing!')\n",
    "    elif dataset.split('-')[0] == 'ff' and (ffpp_df_path is None or ffpp_faces_dir is None):\n",
    "        raise RuntimeError('Specify DataFrame and directory for FF++ faces for testing!')\n",
    "splits = split.make_splits(dfdc_df=dfdc_df_path, ffpp_df=ffpp_df_path, dfdc_dir=dfdc_faces_dir,\n",
    "                           ffpp_dir=ffpp_faces_dir, dbs={'train': test_sets, 'val': test_sets, 'test': test_sets})\n",
    "train_dfs = [splits['train'][db][0] for db in splits['train']]\n",
    "train_roots = [splits['train'][db][1] for db in splits['train']]\n",
    "val_roots = [splits['val'][db][1] for db in splits['val']]\n",
    "val_dfs = [splits['val'][db][0] for db in splits['val']]\n",
    "test_dfs = [splits['test'][db][0] for db in splits['test']]\n",
    "test_roots = [splits['test'][db][1] for db in splits['test']]\n",
    "\n",
    "# Output paths\n",
    "out_folder = results_dir.joinpath(model_name)\n",
    "out_folder.mkdir(mode=0o775, parents=True, exist_ok=True)\n",
    "\n",
    "# Samples selection\n",
    "if max_num_videos_per_label and max_num_videos_per_label > 0:\n",
    "    dfs_out_train = [select_videos(df, max_num_videos_per_label) for df in train_dfs]\n",
    "    dfs_out_val = [select_videos(df, max_num_videos_per_label) for df in val_dfs]\n",
    "    dfs_out_test = [select_videos(df, max_num_videos_per_label) for df in test_dfs]\n",
    "else:\n",
    "    dfs_out_train = train_dfs\n",
    "    dfs_out_val = val_dfs\n",
    "    dfs_out_test = test_dfs\n",
    "\n",
    "# Extractions list\n",
    "extr_list = []\n",
    "# Append train and validation set first\n",
    "if 'train' in test_splits:\n",
    "    for idx, dataset in enumerate(test_sets):\n",
    "        extr_list.append(\n",
    "            (dfs_out_train[idx], out_folder.joinpath(dataset + '_train.pkl'), train_roots[idx], dataset + ' TRAIN')\n",
    "        )\n",
    "if 'val' in test_splits:\n",
    "    for idx, dataset in enumerate(test_sets):\n",
    "        extr_list.append(\n",
    "            (dfs_out_val[idx], out_folder.joinpath(dataset + '_val.pkl'), val_roots[idx], dataset + ' VAL')\n",
    "        )\n",
    "if 'test' in test_splits:\n",
    "    for idx, dataset in enumerate(test_sets):\n",
    "        extr_list.append(\n",
    "            (dfs_out_test[idx], out_folder.joinpath(dataset + '_test.pkl'), test_roots[idx], dataset + ' TEST')\n",
    "        )\n",
    "\n",
    "for df, df_path, df_root, tag in extr_list:\n",
    "    if override or not df_path.exists():\n",
    "        print('\\n##### PREDICT VIDEOS FROM {} #####'.format(tag))\n",
    "        print('Real frames: {}'.format(sum(df['label'] == False)))\n",
    "        print('Fake frames: {}'.format(sum(df['label'] == True)))\n",
    "        print('Real videos: {}'.format(df[df['label'] == False]['video'].nunique()))\n",
    "        print('Fake videos: {}'.format(df[df['label'] == True]['video'].nunique()))\n",
    "        dataset_out = process_dataset(root=df_root, df=df, net=net, criterion=criterion,\n",
    "                                      patch_size=patch_size,\n",
    "                                      face_policy=face_policy, transformer=test_transformer,\n",
    "                                      batch_size=batch_size,\n",
    "                                      num_workers=num_workers, device=device, )\n",
    "        df['score'] = dataset_out['score'].astype(np.float32)\n",
    "        df['loss'] = dataset_out['loss'].astype(np.float32)\n",
    "        print('Saving results to: {}'.format(df_path))\n",
    "        df.to_pickle(str(df_path))\n",
    "\n",
    "        if debug:\n",
    "            plt.figure()\n",
    "            plt.title(tag)\n",
    "            plt.hist(df[df.label == True].score, bins=100, alpha=0.6, label='FAKE frames')\n",
    "            plt.hist(df[df.label == False].score, bins=100, alpha=0.6, label='REAL frames')\n",
    "            plt.legend()\n",
    "\n",
    "        del (dataset_out)\n",
    "        del (df)\n",
    "        gc.collect()\n",
    "\n",
    "if debug:\n",
    "    plt.show()\n",
    "\n",
    "print('Completed!')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OjStcbso0WTQ",
    "outputId": "1dc874f3-b9f4-4f7c-81cc-62518e28038f"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading model...\n",
      "<All keys matched successfully>\n",
      "Model loaded!\n",
      "Loading data...\n",
      "\n",
      "##### PREDICT VIDEOS FROM ff-c23-720-140-140 VAL #####\n",
      "Real frames: 398\n",
      "Fake frames: 6290\n",
      "Real videos: 43\n",
      "Fake videos: 631\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r  0%|          | 0/27 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "100%|██████████| 27/27 [06:51<00:00, 15.24s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving results to: results/net-Xception_traindb-ff-c23-720-140-140_face-scale_size-224_seed-41_bestval/ff-c23-720-140-140_val.pkl\n",
      "\n",
      "##### PREDICT VIDEOS FROM ff-c23-720-140-140 TEST #####\n",
      "Real frames: 332\n",
      "Fake frames: 5937\n",
      "Real videos: 35\n",
      "Fake videos: 595\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r  0%|          | 0/25 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "100%|██████████| 25/25 [05:46<00:00, 13.87s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving results to: results/net-Xception_traindb-ff-c23-720-140-140_face-scale_size-224_seed-41_bestval/ff-c23-720-140-140_test.pkl\n",
      "Completed!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "import shutil\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "source_folder = \"/content/CS599-Fake-Video-Detection/results\"\n",
    "target_folder = \"/content/drive/MyDrive/CS599/results\"\n",
    "\n",
    "shutil.copytree(source_folder, target_folder)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "_sSt83480YHr",
    "outputId": "75e925a0-8fbf-41f7-e9ea-e9dc10d9ce3d"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/content/drive/MyDrive/CS599/results'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  }
 ]
}
